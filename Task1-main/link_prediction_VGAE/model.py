import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import numpy as np

import args

class VGAE(nn.Module):
	def __init__(self, adj):
		super(VGAE,self).__init__()
		self.base_gcn = GraphConvSparse(args.input_dim, args.hidden1_dim, adj)  # ç¬¬ä¸€å±‚å›¾å·ç§¯
		self.gcn_mean = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)  # å‡å€¼
		self.gcn_logstddev = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)  # å¯¹æ•°æ ‡å‡†å·®

	def encode(self, X):
		hidden = self.base_gcn(X)  # muå’Œsigmaçš„ç¬¬ä¸€å±‚å·ç§¯å±‚å…±äº«
		self.mean = self.gcn_mean(hidden)  # hiddenè¿‡å¦ä¸€å±‚å¾—åˆ°mu
		self.logstd = self.gcn_logstddev(hidden)  # hiddenè¿‡å¦ä¸€å±‚å¾—åˆ°sigma
		gaussian_noise = torch.randn(X.size(0), args.hidden2_dim)
		sampled_z = gaussian_noise*torch.exp(self.logstd) + self.mean
		return sampled_z

	def forward(self, X):
		Z = self.encode(X)  # ç¼–ç å¾—åˆ°éšå˜é‡
		A_pred = dot_product_decode(Z)  # è¿™é‡Œçš„A_predå–å€¼èŒƒå›´æœªå¿…æ˜¯[0,1]
		return A_pred

class GraphConvSparse(nn.Module):
	"""
	å°†è¾“å…¥ç‰¹å¾ğ‘‹è½¬æ¢ä¸ºæ–°çš„åµŒå…¥è¡¨ç¤ºğ»
	H = \sigma(\hat{A}XW_0)
	"""
	def __init__(self, input_dim, output_dim, adj, activation = F.relu, **kwargs):
		super(GraphConvSparse, self).__init__(**kwargs)
		self.weight = glorot_init(input_dim, output_dim) 
		self.adj = adj
		self.activation = activation

	def forward(self, inputs):
		x = inputs
		x = torch.mm(x,self.weight)
		x = torch.mm(self.adj, x)
		outputs = self.activation(x)
		return outputs


def dot_product_decode(Z):
	A_pred = torch.sigmoid(torch.matmul(Z,Z.t()))
	return A_pred

def glorot_init(input_dim, output_dim):
	init_range = np.sqrt(6.0/(input_dim + output_dim))
	initial = torch.rand(input_dim, output_dim)*2*init_range - init_range
	return nn.Parameter(initial)


class GAE(nn.Module):
	"""
	GAE: Graph Auto-Encoder
	åŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨
	"""
	def __init__(self,adj):
		super(GAE,self).__init__()
		# ä¸¤å±‚å·ç§¯å±‚
		self.base_gcn = GraphConvSparse(args.input_dim, args.hidden1_dim, adj)
		self.gcn_mean = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)

	def encode(self, X):
		# ç¬¬ä¸€å±‚å·ç§¯æ˜¯ç¼–ç å™¨
		hidden = self.base_gcn(X)
		z = self.mean = self.gcn_mean(hidden)
		return z

	def forward(self, X):
		# ç¬¬äºŒå±‚å·ç§¯æ˜¯è§£ç å™¨
		Z = self.encode(X)
		A_pred = dot_product_decode(Z)
		return A_pred
		

# class GraphConv(nn.Module):
# 	def __init__(self, input_dim, hidden_dim, output_dim):
# 		super(VGAE,self).__init__()
# 		self.base_gcn = GraphConvSparse(args.input_dim, args.hidden1_dim, adj)
# 		self.gcn_mean = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)
# 		self.gcn_logstddev = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)

# 	def forward(self, X, A):
# 		out = A*X*self.w0
# 		out = F.relu(out)
# 		out = A*X*self.w0
# 		return out