{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, average_precision_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate features for each pair of nodes\n",
    "def common_neighbors(x, y, subgraph):\n",
    "    return len(list(nx.common_neighbors(subgraph, x, y)))\n",
    "\n",
    "def adamic_adar(x, y, subgraph):\n",
    "    common_neighbors_list = list(nx.common_neighbors(subgraph, x, y))\n",
    "    if len(common_neighbors_list) == 0:\n",
    "        return 1e-6  # 如果没有共同邻居，返回一个小的正数\n",
    "    return sum(1/np.log(len(list(subgraph.neighbors(z)))) for z in common_neighbors_list)\n",
    "\n",
    "def jaccard_coefficient(x, y, subgraph):\n",
    "    common_neighbors_list = list(nx.common_neighbors(subgraph, x, y))\n",
    "    if len(common_neighbors_list) == 0:\n",
    "        return 1e-6  # 如果没有共同邻居，返回一个小的正数\n",
    "    return len(common_neighbors_list) / (len(set(subgraph.neighbors(x))) + len(set(subgraph.neighbors(y))) - len(common_neighbors_list))\n",
    "\n",
    "def resource_allocation(x, y, subgraph):\n",
    "    common_neighbors_list = list(nx.common_neighbors(subgraph, x, y))\n",
    "    if len(common_neighbors_list) == 0:\n",
    "        return 1e-6  # 如果没有共同邻居，返回一个小的正数\n",
    "    return sum(1/len(list(subgraph.neighbors(z))) for z in common_neighbors_list)\n",
    "\n",
    "def preferential_attachment(x, y, subgraph):\n",
    "    neighbors_x = list(subgraph.neighbors(x))\n",
    "    neighbors_y = list(subgraph.neighbors(y))\n",
    "    if len(neighbors_x) == 0 or len(neighbors_y) == 0:\n",
    "        return 1e-6  # 如果任一节点没有邻居，返回一个小的正数\n",
    "    return len(neighbors_x) * len(neighbors_y)\n",
    "\n",
    "def dice_coefficient(x, y, subgraph):\n",
    "    neighbors_x = set(subgraph.neighbors(x))\n",
    "    neighbors_y = set(subgraph.neighbors(y))\n",
    "    \n",
    "    intersection_size = len(neighbors_x & neighbors_y)\n",
    "    if len(neighbors_x) + len(neighbors_y) == 0:\n",
    "        return 1e-6  # 如果邻居集合为空，返回一个小的正数\n",
    "    return 2 * intersection_size / (len(neighbors_x) + len(neighbors_y))\n",
    "\n",
    "def cosine_similarity(x, y, subgraph):\n",
    "    neighbors_x = set(subgraph.neighbors(x))\n",
    "    neighbors_y = set(subgraph.neighbors(y))\n",
    "    \n",
    "    intersection_size = len(neighbors_x & neighbors_y)\n",
    "    if len(neighbors_x) == 0 or len(neighbors_y) == 0:\n",
    "        return 1e-6  # 如果任一节点没有邻居，返回一个小的正数\n",
    "    return intersection_size / (np.sqrt(len(neighbors_x)) * np.sqrt(len(neighbors_y)))\n",
    "\n",
    "def pearson_correlation(x, y, subgraph):\n",
    "    degree_x = len(list(subgraph.neighbors(x)))\n",
    "    degree_y = len(list(subgraph.neighbors(y)))\n",
    "\n",
    "    neighbors_x = list(subgraph.neighbors(x))\n",
    "    neighbors_y = list(subgraph.neighbors(y))\n",
    "    \n",
    "    if len(neighbors_x) == 0 or len(neighbors_y) == 0:\n",
    "        return 1e-6  # 如果任一节点没有邻居，返回一个小的正数\n",
    "    \n",
    "    mean_degree_x = np.mean([len(list(subgraph.neighbors(neighbor))) for neighbor in neighbors_x])\n",
    "    mean_degree_y = np.mean([len(list(subgraph.neighbors(neighbor))) for neighbor in neighbors_y])\n",
    "    \n",
    "    # Pearson formula\n",
    "    numerator = (degree_x - mean_degree_x) * (degree_y - mean_degree_y)\n",
    "    denominator = np.sqrt((degree_x - mean_degree_x)**2 * (degree_y - mean_degree_y)**2)\n",
    "\n",
    "    # Return Pearson Correlation (avoid division by zero)\n",
    "    if denominator != 0:\n",
    "        return numerator / denominator\n",
    "    else:\n",
    "        return 1e-6  # 如果没有变化，返回一个小的正数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate ROC AUC and Average Precision (AP)\n",
    "def evaluate_model(model, X_test_scaled, y_test):\n",
    "    # Get predicted probabilities\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # We are interested in the probability for class '1'\n",
    "\n",
    "    # Calculate ROC AUC and AP\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    ap = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    return roc_auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "\n",
    "all_data = []\n",
    "all_edges = []\n",
    "\n",
    "for root,dirs,files in os.walk('./cora'):\n",
    "    for file in files:\n",
    "        if '.content' in file:\n",
    "            with open(os.path.join(root,file),'r') as f:\n",
    "                all_data.extend(f.read().splitlines())\n",
    "        elif 'cites' in file:\n",
    "            with open(os.path.join(root,file),'r') as f:\n",
    "                all_edges.extend(f.read().splitlines())\n",
    "\n",
    "                \n",
    "#Shuffle the data because the raw data is ordered based on the label\n",
    "random_state = 42\n",
    "all_data = shuffle(all_data,random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "\n",
    "all_data = []\n",
    "all_edges = []\n",
    "\n",
    "for root,dirs,files in os.walk('./cora'):\n",
    "    for file in files:\n",
    "        if '.content' in file:\n",
    "            with open(os.path.join(root,file),'r') as f:\n",
    "                all_data.extend(f.read().splitlines())\n",
    "        elif 'cites' in file:\n",
    "            with open(os.path.join(root,file),'r') as f:\n",
    "                all_edges.extend(f.read().splitlines())\n",
    "\n",
    "                \n",
    "#Shuffle the data because the raw data is ordered based on the label\n",
    "random_state = 42\n",
    "all_data = shuffle(all_data,random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape:  (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "#parse the data\n",
    "labels = []\n",
    "nodes = []\n",
    "features = []\n",
    "\n",
    "for i,data in enumerate(all_data):\n",
    "    elements = data.split('\\t')\n",
    "    labels.append(elements[-1])\n",
    "    features.append(elements[1:-1])\n",
    "    nodes.append(elements[0])\n",
    "\n",
    "features = np.array(features,dtype=int)\n",
    "a = features.shape[0] #the number of nodes\n",
    "b = features.shape[1] #the size of node features\n",
    "\n",
    "print('features shape: ', features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of nodes (a):  2708\n",
      "\n",
      "Number of features (b) of each node:  1433\n",
      "\n",
      "Categories:  {'Case_Based', 'Theory', 'Rule_Learning', 'Neural_Networks', 'Probabilistic_Methods', 'Genetic_Algorithms', 'Reinforcement_Learning'}\n",
      "\n",
      "Number of classes:  7\n"
     ]
    }
   ],
   "source": [
    "#parse the edge\n",
    "edge_list=[]\n",
    "for edge in all_edges:\n",
    "    e = edge.split('\\t')\n",
    "    edge_list.append((e[0],e[1]))\n",
    "\n",
    "print('\\nNumber of nodes (a): ', a)\n",
    "print('\\nNumber of features (b) of each node: ', b)\n",
    "print('\\nCategories: ', set(labels))\n",
    "\n",
    "num_classes = len(set(labels))\n",
    "print('\\nNumber of classes: ', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 2708 nodes and 5278 edges\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate positive (existing edges) and negative (non-existing edges) samples\n",
    "edges = list(G.edges())  # Positive edges\n",
    "non_edges = list(nx.non_edges(G))  # Negative edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5278\n",
      "3660000\n"
     ]
    }
   ],
   "source": [
    "print(len(edges))\n",
    "print(len(non_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating train features: 100%|██████████| 7917/7917 [00:00<00:00, 20847.01pair/s]\n",
      "Calculating test features: 100%|██████████| 2639/2639 [00:00<00:00, 29613.21pair/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes ROC AUC: 0.6799151298173485\n",
      "Naive Bayes AP: 0.7892051581022573\n",
      "SVM ROC AUC: 0.7092256371874421\n",
      "SVM AP: 0.8140206988162942\n",
      "Logistic Regression ROC AUC: 0.8517545432910362\n",
      "Logistic Regression AP: 0.8847126999345049\n",
      "MLP ROC AUC: 0.9389925143625136\n",
      "MLP AP: 0.9242845433081626\n",
      "KNN ROC AUC: 0.9076180243941416\n",
      "KNN AP: 0.8761398486946804\n",
      "Decision Tree ROC AUC: 0.9298714415629357\n",
      "Decision Tree AP: 0.9039295605905624\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample the negative edges to match the number of positive edges\n",
    "non_edges_sample = np.random.choice(len(non_edges), size=len(edges), replace=False)\n",
    "non_edges_sample = [non_edges[i] for i in non_edges_sample]\n",
    "\n",
    "# Combine positive and negative samples\n",
    "samples = edges + non_edges_sample\n",
    "labels = [1] * len(edges) + [0] * len(non_edges_sample)\n",
    "\n",
    "# Step 2.1: Shuffle the samples and labels\n",
    "# Combine the samples and labels into a single list of tuples (sample, label)\n",
    "data = list(zip(samples, labels))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Unzip the shuffled data back into samples and labels\n",
    "samples, labels = zip(*data)\n",
    "\n",
    "# Step 3: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(samples, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Step 4: Create subgraphs for train and test separately\n",
    "# Create subgraph for the training set (only include edges from the training set)\n",
    "train_subgraph = G.edge_subgraph(X_train).copy()\n",
    "\n",
    "# Create subgraph for the test set (only include edges from the test set)\n",
    "test_subgraph = G.edge_subgraph(X_test).copy()\n",
    "\n",
    "# Step 5: Calculate features only for training set and test set separately\n",
    "\n",
    "# Train features\n",
    "train_features = []\n",
    "for edge in tqdm(X_train, desc=\"Calculating train features\", unit=\"pair\"):\n",
    "    x, y = edge\n",
    "    train_features.append([\n",
    "        common_neighbors(x, y, train_subgraph),\n",
    "        adamic_adar(x, y, train_subgraph),\n",
    "        jaccard_coefficient(x, y, train_subgraph),\n",
    "        resource_allocation(x, y, train_subgraph),\n",
    "        preferential_attachment(x, y, train_subgraph),\n",
    "        dice_coefficient(x, y, train_subgraph),     \n",
    "        cosine_similarity(x, y, train_subgraph),\n",
    "        pearson_correlation(x, y, train_subgraph)     \n",
    "    ])\n",
    "\n",
    "# Test features\n",
    "test_features = []\n",
    "for edge in tqdm(X_test, desc=\"Calculating test features\", unit=\"pair\"):\n",
    "    x, y = edge\n",
    "    test_features.append([\n",
    "        common_neighbors(x, y, test_subgraph),\n",
    "        adamic_adar(x, y, test_subgraph),\n",
    "        jaccard_coefficient(x, y, test_subgraph),\n",
    "        resource_allocation(x, y, test_subgraph),\n",
    "        preferential_attachment(x, y, test_subgraph),\n",
    "        dice_coefficient(x, y, test_subgraph),     \n",
    "        cosine_similarity(x, y, test_subgraph),\n",
    "        pearson_correlation(x, y, test_subgraph)     \n",
    "    ])\n",
    "\n",
    "# Convert features and labels into DataFrame for easier handling\n",
    "X_train = np.array(train_features)\n",
    "X_test = np.array(test_features)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Step 6: Standardize the features (fit on training data only)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Only transform the test data\n",
    "\n",
    "# 1. Naive Bayes (GaussianNB)\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train)  # Note: Naive Bayes usually doesn't require scaling\n",
    "nb_roc_auc, nb_ap = evaluate_model(nb_model, X_test_scaled, y_test)\n",
    "print(\"Naive Bayes ROC AUC:\", nb_roc_auc)\n",
    "print(\"Naive Bayes AP:\", nb_ap)\n",
    "\n",
    "# 2. Support Vector Machine (SVM)\n",
    "svm_model = SVC(kernel='rbf', gamma='scale', random_state=42, probability=True)  # Ensure probability=True to get probas\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_roc_auc, svm_ap = evaluate_model(svm_model, X_test_scaled, y_test)\n",
    "print(\"SVM ROC AUC:\", svm_roc_auc)\n",
    "print(\"SVM AP:\", svm_ap)\n",
    "\n",
    "# 3. Logistic Regression\n",
    "log_reg_model = LogisticRegression(random_state=42)\n",
    "log_reg_model.fit(X_train_scaled, y_train)\n",
    "log_reg_roc_auc, log_reg_ap = evaluate_model(log_reg_model, X_test_scaled, y_test)\n",
    "print(\"Logistic Regression ROC AUC:\", log_reg_roc_auc)\n",
    "print(\"Logistic Regression AP:\", log_reg_ap)\n",
    "\n",
    "# 4. Multi-layer Perceptron (MLP)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "mlp_roc_auc, mlp_ap = evaluate_model(mlp_model, X_test_scaled, y_test)\n",
    "print(\"MLP ROC AUC:\", mlp_roc_auc)\n",
    "print(\"MLP AP:\", mlp_ap)\n",
    "\n",
    "# 5. K-Nearest Neighbors (KNN)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)  # 选择合适的k值\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_roc_auc, knn_ap = evaluate_model(knn_model, X_test_scaled, y_test)\n",
    "print(\"KNN ROC AUC:\", knn_roc_auc)\n",
    "print(\"KNN AP:\", knn_ap)\n",
    "\n",
    "# 6. Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "dt_roc_auc, dt_ap = evaluate_model(dt_model, X_test_scaled, y_test)\n",
    "print(\"Decision Tree ROC AUC:\", dt_roc_auc)\n",
    "print(\"Decision Tree AP:\", dt_ap)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
